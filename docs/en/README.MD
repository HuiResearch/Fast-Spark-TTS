# Project Documentation Overview

Welcome to the `FlashTTS` documentation. This project is divided into three main sections: **Getting
Started (`get_started`)**, **Model Inference (`inference`)**, and **Server Deployment (`server`)**.

---

## ğŸ“¦ Directory Structure

```
â”œâ”€â”€ get_started
â”‚   â”œâ”€â”€ installation.md      # Installation Guide
â”‚   â””â”€â”€ quick_start.md       # Quick Start Guide
â”œâ”€â”€ inference
â”‚   â”œâ”€â”€ auto-engine.md       # Auto Inference Engine Guide
â”‚   â”œâ”€â”€ mega-tts.md          # Mega TTS Model Documentation
â”‚   â”œâ”€â”€ orpheus-tts.md       # Orpheus TTS Model Documentation
â”‚   â””â”€â”€ spark-tts.md         # Spark TTS Model Documentation
â””â”€â”€ server
    â”œâ”€â”€ client.md            # Client Usage Guide
    â””â”€â”€ server.md            # Server Deployment Guide
```

## ğŸ› ï¸ Getting Started

- [Installation Guide](get_started/installation.md): Learn how to install project dependencies, perform basic setup, and
  get started quickly.
- [Quick Start Guide](get_started/quick_start.md): A quick-start guide to using command-line tools to perform speech synthesis

## ğŸ§  Inference Engines and Models

- [mega-tts](inference/mega-tts.md): Configuration and usage of the Mega TTS model.
- [orpheus-tts](inference/orpheus-tts.md): Detailed documentation for the Orpheus TTS model.
- [spark-tts](inference/spark-tts.md): Inference process and features of the Spark TTS model.
- [auto-engine](inference/auto-engine.md): Introduction to the project's automatic inference engine module.

## ğŸŒ Deploying the Service

- [server](server/server.md): Steps to deploy the server, including runtime environment and deployment commands.
- [client](server/client.md): How to call the model APIs, including example requests and response formats.